{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import ArxivLoader\n",
    "from openai import OpenAI\n",
    "import regex as re\n",
    "import requests\n",
    "from upload_notion import NotionDatabase, transform_date\n",
    "from datetime import datetime\n",
    "import arxiv\n",
    "import json\n",
    "client = arxiv.Client()\n",
    "llm = OpenAI(api_key=\"sk-\")\n",
    "database_id = \"\"\n",
    "notion_key = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(llm, docs):\n",
    "    print(\"Extracting keywords from summary...\")\n",
    "    try:\n",
    "        script = docs[0].page_content[:10000]\n",
    "    except:\n",
    "        script = docs[1].page_content[:10000]\n",
    "    template = \"\"\"Choose 5 most important and well represent paper keywords of the following paper:\n",
    "    \"{paper}\"\n",
    "    KEYWORDS:\n",
    "    \"\"\"\n",
    "    question = \"Choose 5 most important and well represent paper keywords of the following paper:\" + script + \"\"\"\n",
    "    example:\n",
    "    keyword 1, keyword 2, keyword 3, keyword 4, keyword 5.\n",
    "    separate keywords with comma and space.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    answer = llm.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": template},\n",
    "            {\"role\": \"assistant\", \"content\": question},\n",
    "            {\"role\": \"user\", \"content\": \"KEYWORDS:\"},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "    answer = answer.choices[0].message.content\n",
    "    keywords = []\n",
    "    try:\n",
    "        for i in range(5):\n",
    "            #split by comma or carriage return\n",
    "            keywords.append(re.split(\",|\\n\", answer)[i].strip())\n",
    "    except:\n",
    "        print(\"Warning: Less than 5 keywords were extracted.\")\n",
    "    #print(keywords)\n",
    "    #만약 키워드의 시작이 1. 2. 3. 4. 5. 이런식으로 되어있으면 제거\n",
    "    for i in range(5):\n",
    "        if re.match(r\"^\\d\\.\", keywords[i]):\n",
    "            keywords[i] = keywords[i][2:]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary(llm, docs):\n",
    "    print(\"Extracting summary from paper...\")\n",
    "    try:\n",
    "        script = docs[0].page_content[:10000]\n",
    "    except:\n",
    "        script = docs[1].page_content[:10000]\n",
    "    template = \"\"\"\n",
    "    SUMMARY:\n",
    "    \"\"\"\n",
    "    question = \"Write a summary of the following paper as korean :\" + script\n",
    "    answer = llm.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": question},\n",
    "            {\"role\": \"assistant\", \"content\": template},\n",
    "            {\"role\": \"user\", \"content\": \"SUMMARY:\"},\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "    return answer.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(init):\n",
    "    docs = ArxivLoader(query=init, load_max_docs=3).load()\n",
    "    print(\"Getting metadata from arxiv...\")\n",
    "    pdf_URL = f\"https://arxiv.org/pdf/{init}.pdf\"\n",
    "    response = requests.get(f\"https://arxiv.org/abs/{init}\")\n",
    "    taxonomy = re.search(r'<span class=\"primary-subject\">(.*?)</span>', response.text).group(1)\n",
    "    return docs, pdf_URL, taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting metadata from arxiv...\n",
      "Processing 2311.11628v1... Title: Incorporating LLM Priors into Tabular Learners\n",
      "Uploading to Notion...\n",
      "Getting metadata from arxiv...\n",
      "Processing 2311.16267v2... Title: Novel Preprocessing Technique for Data Embedding in Engineering Code Generation Using Large Language Model\n",
      "Uploading to Notion...\n",
      "Getting metadata from arxiv...\n",
      "Processing 2312.12112v2... Title: Curated LLM: Synergy of LLMs and Data Curation for tabular augmentation in ultra low-data regimes\n",
      "Uploading to Notion...\n",
      "Getting metadata from arxiv...\n",
      "Processing 2311.01918v1... Title: Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review\n",
      "Uploading to Notion...\n",
      "Getting metadata from arxiv...\n",
      "Processing 2402.09939v1... Title: Generative AI in the Construction Industry: A State-of-the-art Analysis\n",
      "Uploading to Notion...\n",
      "Getting metadata from arxiv...\n",
      "Processing 2402.12869v1... Title: Exploring the Impact of Table-to-Text Methods on Augmenting LLM-based Question Answering with Domain Hybrid Data\n",
      "Uploading to Notion...\n",
      "Getting metadata from arxiv...\n",
      "Processing 2401.13085v1... Title: IndiText Boost: Text Augmentation for Low Resource India Languages\n",
      "Uploading to Notion...\n",
      "Getting metadata from arxiv...\n",
      "Processing 2402.14744v1... Title: Large Language Models as Urban Residents: An LLM Agent Framework for Personal Mobility Generation\n",
      "Uploading to Notion...\n",
      "Getting metadata from arxiv...\n",
      "Processing 2310.05318v2... Title: Resolving the Imbalance Issue in Hierarchical Disciplinary Topic Inference via LLM-based Data Augmentation\n",
      "Uploading to Notion...\n",
      "Getting metadata from arxiv...\n",
      "Processing 2312.09039v2... Title: TAP4LLM: Table Provider on Sampling, Augmenting, and Packing Semi-structured Data for Large Language Model Reasoning\n",
      "Uploading to Notion...\n"
     ]
    }
   ],
   "source": [
    "term = \"Table data augmentation, Categorical data, LLM, Tablular Data Generation\"\n",
    "#Relevance, LastUpdatedDate, SubmittedDate\n",
    "sort_by = \"relevance\"\n",
    "\n",
    "\n",
    "search = arxiv.Search(\n",
    "  query = term,\n",
    "  max_results = 10,\n",
    "  sort_by = arxiv.SortCriterion.Relevance if sort_by == \"relevance\" else arxiv.SortCriterion.LastUpdatedDate if sort_by == \"last_updated_date\" else arxiv.SortCriterion.SubmittedDate\n",
    "  \n",
    ")\n",
    "result = client.results(search)\n",
    "all_results = list(result)\n",
    "if len(all_results) == 0:\n",
    "    print(\"No results found.\")\n",
    "    exit(0)\n",
    "\n",
    "for i in range(len(all_results)):\n",
    "    init = all_results[i].__dict__[\"entry_id\"].split(\"/\")[-1]\n",
    "    docs, pdf_URL, taxonomy = get_metadata(init)\n",
    "    print(f\"Processing {init}... Title: {docs[0].metadata['Title']}\")\n",
    "    cache = json.load(open(\"cache.json\", \"r\"))\n",
    "    if init in cache:\n",
    "        page_values = cache[init]\n",
    "    else:\n",
    "        try:\n",
    "            summary = get_summary(llm, docs)\n",
    "            keywords = get_keywords(llm, docs)\n",
    "            page_values = {\n",
    "            'Search term and sort by': \", \".join([term, sort_by]),\n",
    "            'Taxonomy': taxonomy,\n",
    "            'Keywords': keywords,\n",
    "            'Summary': summary.strip(),\n",
    "            'Title': docs[0].metadata['Title'],\n",
    "            'Published': transform_date(datetime.strftime(datetime.strptime(docs[0].metadata['Published'], '%Y-%m-%d'), '%Y%m%d%H%M%S')),\n",
    "            'URL': pdf_URL,\n",
    "            }\n",
    "            cache[init] = page_values\n",
    "            json.dump(cache, open(\"cache.json\", \"w\"))\n",
    "        except:\n",
    "            print(f\"Error: Unable to extract summary and keywords. id: {init}\")\n",
    "            continue\n",
    "    notion_db = None #초기화하지 않으면 Keyword에서 중복되는 문제가 발생함. Notion측 문제로 보임.\n",
    "    print(\"Uploading to Notion...\")\n",
    "    notion_db = NotionDatabase(database_id, notion_key)\n",
    "    notion_db.upload_page_values(page_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
